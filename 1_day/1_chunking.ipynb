{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND/KuncMDp8N7MUoBcIITA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edquestofficial/Gen-AI-Cohort/blob/main/1_day/1_chunking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRcTzCjZheq-",
        "outputId": "cf37f49a-6505-48c9-f7f8-7c05202d3fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m174.1/232.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "VnSMfgjHhoWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def mount_drive():\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW1aEQpQhrKR",
        "outputId": "559b34b4-4c38-4584-f1b2-fd30f78cc737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_size_text_chunker(text, chunk_size):\n",
        "    \"\"\"\n",
        "    Splits text into chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be chunked.\n",
        "        chunk_size (int): The maximum size of each chunk.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []  # List to store the text chunks\n",
        "    words = text.split()  # Split the text into words\n",
        "\n",
        "    current_chunk = []  # Store words for the current chunk\n",
        "    current_chunk_length = 0  # Total length of words in the current chunk\n",
        "\n",
        "    # Iterate through each word in the text\n",
        "    for word in words:\n",
        "        # Check if adding the current word to the chunk exceeds the chunk size\n",
        "        if current_chunk_length + len(word) + 1 <= chunk_size:\n",
        "            current_chunk.append(word)\n",
        "            current_chunk_length += len(word) + 1\n",
        "        else:\n",
        "            # Add the current chunk to the list of chunks\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            # Start a new chunk with the current word\n",
        "            current_chunk = [word]\n",
        "            current_chunk_length = len(word)\n",
        "\n",
        "    # Append the remaining chunk if it's not empty\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "YxdNwkBLhrfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_size_chunking(page,pdf_meta, chunk_size):\n",
        "  # Initialize an empty list to store the text chunks\n",
        "  text_chunks = []\n",
        "  all_dfs = []\n",
        "  text = page.extract_text()\n",
        "\n",
        "  # Remove blank lines and tables\n",
        "  text = re.sub(r'^\\s*$\\n', '', text, flags=re.MULTILINE)  # Remove blank lines\n",
        "  text = re.sub(r'\\|?\\s*\\|\\s*\\|?', '', text)  # Remove table lines\n",
        "  words = text.split()  # Split the text into words\n",
        "  print(f\"length of text: {len(words)}\")\n",
        "\n",
        "  text_chunks = fix_size_text_chunker(text, chunk_size)\n",
        "\n",
        "  # Create a DataFrame to store the chunks and page title\n",
        "  data = {'Title': [], 'Chunk Text': []}\n",
        "  for idx, chunk in enumerate(text_chunks):\n",
        "    data['Title'].append(pdf_meta.author)\n",
        "    data['Chunk Text'].append(chunk)\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "KE2BKDS_iAmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentences_from_text(text):\n",
        "    \"\"\"\n",
        "    Extracts sentences from the input text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text containing multiple sentences.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of sentences extracted from the text.\n",
        "    \"\"\"\n",
        "    sentences = text.split('\\n')  # Split the text into lines to extract sentences\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "9wlKHu6ZiC4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_chunking(pdf_page, pdf_meta):\n",
        "    \"\"\"\n",
        "    Extracts and chunks sentences from a PDF page.\n",
        "\n",
        "    Args:\n",
        "        pdf_page (str): The text extracted from the PDF page.\n",
        "        pdf_meta (str): Metadata of the PDF.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing page title and chunked text.\n",
        "    \"\"\"\n",
        "    # Extract text from the PDF page\n",
        "    text = pdf_page.extract_text()\n",
        "\n",
        "    # Remove blank lines from the text\n",
        "    text = re.sub(r'^\\s*$\\n', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove table lines from the text\n",
        "    text = re.sub(r'\\|?\\s*\\|\\s*\\|?', '', text)\n",
        "\n",
        "    # Split the text into sentences\n",
        "    sentence_chunks = extract_sentences_from_text(text)\n",
        "\n",
        "    # Create a dictionary to store the chunks and page title\n",
        "    data = {'Title': [], 'Chunk Text': []}\n",
        "\n",
        "    # Add page title and chunked text to the dictionary\n",
        "    for idx, chunk in enumerate(sentence_chunks):\n",
        "        data['Title'].append(pdf_meta.author)\n",
        "        data['Chunk Text'].append(chunk)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "xUtaEF46iEvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_page(pdf_page):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF page.\n",
        "\n",
        "    Args:\n",
        "        pdf_page: The PDF page to extract text from.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted text from the PDF page.\n",
        "    \"\"\"\n",
        "    return pdf_page.extract_text()"
      ],
      "metadata": {
        "id": "tiNWIC4njykQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def page_chunking(pdf_page, pdf_meta):\n",
        "    \"\"\"\n",
        "    Extracts and chunks text from a PDF page.\n",
        "\n",
        "    Args:\n",
        "        pdf_page: The PDF page to extract text from.\n",
        "        pdf_meta: Metadata of the PDF.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing page title and chunked text.\n",
        "    \"\"\"\n",
        "    # Extract text from the PDF page\n",
        "    text = extract_text_from_page(pdf_page)\n",
        "\n",
        "    # Remove blank lines and table lines from the text\n",
        "    cleaned_text = re.sub(r'^\\s*$\\n', '', text, flags=re.MULTILINE)  # Remove blank lines\n",
        "    cleaned_text = re.sub(r'\\|?\\s*\\|\\s*\\|?', '', cleaned_text)  # Remove table lines\n",
        "\n",
        "    # Create a dictionary to store the chunks and page title\n",
        "    data = {'Title': [], 'Chunk Text': []}\n",
        "\n",
        "    # Add page title and chunked text to the dictionary\n",
        "    data['Title'].append(pdf_meta.author)\n",
        "    data['Chunk Text'].append(cleaned_text)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "Pfp0EbEKiGop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the PDF file\n",
        "pdf_file = open('/content/drive/MyDrive/Gen Ai/upgrad/data/bike2024-2025.pdf', 'rb')\n",
        "fix_all_dfs = []\n",
        "sentence_all_dfs = []\n",
        "page_all_dfs = []\n",
        "chunk_size = 300\n",
        "# Create a PDF reader object\n",
        "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "# Number of pages in the PDF\n",
        "pdf_pages_count = len(pdf_reader.pages)\n",
        "pdf_meta = pdf_reader.metadata\n",
        "print(f\"Number of pages in pdf {pdf_pages_count}\")\n",
        "for page_num in range(pdf_pages_count):\n",
        "  # Extract the text from the current page\n",
        "  text = pdf_reader.pages[page_num].extract_text()\n",
        "  page = pdf_reader.pages[page_num]\n",
        "  fix_chunk_data = fix_size_text_chunker(text, chunk_size)\n",
        "  sentence_chunkdata = sentence_chunking(page, pdf_meta)\n",
        "  page_chunk_data = page_chunking(page, pdf_meta)\n",
        "  # all_dfs.append(pd.DataFrame(data))\n",
        "  print(f\"pange no: {page_num}  && length of df : {len(fix_all_dfs)}\")\n",
        "  fix_all_dfs.append(pd.DataFrame(fix_chunk_data))\n",
        "  sentence_all_dfs.append(pd.DataFrame(sentence_chunkdata))\n",
        "  page_all_dfs.append(pd.DataFrame(page_chunk_data))\n",
        "fix_chunk_df = pd.concat(fix_all_dfs, ignore_index=True)\n",
        "sentence_chunk_df = pd.concat(sentence_all_dfs, ignore_index=True)\n",
        "page_chunk_df = pd.concat(page_all_dfs, ignore_index=True)\n",
        "pdf_file.close()\n",
        "print(f\"fix_chunk_df.shape : {fix_chunk_df.shape}\")\n",
        "print(f\"sentence_chunk_df : {sentence_chunk_df.shape}\")\n",
        "print(f\"page_chunk_df.shape : {page_chunk_df.shape}\")\n",
        "print(\"======================================================\")\n",
        "print(fix_chunk_df.head())\n",
        "print(\"======================================================\")\n",
        "print(sentence_chunk_df.head())\n",
        "print(\"======================================================\")\n",
        "print(page_chunk_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn2RPrTHiKa8",
        "outputId": "8cd45a09-daee-4aac-9454-e83d071356b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages in pdf 2\n",
            "pange no: 0  && length of df : 0\n",
            "pange no: 1  && length of df : 1\n",
            "fix_chunk_df.shape : (44, 1)\n",
            "sentence_chunk_df : (133, 2)\n",
            "page_chunk_df.shape : (2, 2)\n",
            "======================================================\n",
            "                                                   0\n",
            "0  Vehicle Details Policy Details Make ROYAL ENFI...\n",
            "1  ME3U3S5C1HC817838 Invoice No. 204561186201000 ...\n",
            "2  GURGAON TOWER B15 FLAT 804 AVL36GURGAON SECTOR...\n",
            "3  0 47250(`) Own Damage Policy Period Liability ...\n",
            "4  Third Party Liability 1366 Total Basic Premium...\n",
            "======================================================\n",
            "       Title                                         Chunk Text\n",
            "0  HDFC ERGO                     Vehicle Details Policy Details\n",
            "1  HDFC ERGO  Make ROYAL ENFIELD Policy No. 2301 2045 6118 6...\n",
            "2  HDFC ERGO      Model - Variant CLASSIC-Classic 350 Period of\n",
            "3  HDFC ERGO               InsuranceFrom 15 Mar, 2024 00:01 hrs\n",
            "4  HDFC ERGO  Registration No HR-26-DD-2318 To 14 Mar, 2025 ...\n",
            "======================================================\n",
            "       Title                                         Chunk Text\n",
            "0  HDFC ERGO  Vehicle Details Policy Details\\nMake ROYAL ENF...\n",
            "1  HDFC ERGO  Vehicle Details Proposal Details\\nMake ROYAL E...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9iISp0aVmotm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}